{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":328},"id":"CATeQ038iQ96","executionInfo":{"status":"error","timestamp":1674197027350,"user_tz":-330,"elapsed":7705,"user":{"displayName":"Dev Kshatrainfotech","userId":"00316704694514822481"}},"outputId":"e94dea5e-5156-4e70-861b-9c20f7a4fe31"},"execution_count":1,"outputs":[{"output_type":"error","ename":"MessageError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    122\u001b[0m       'TBE_EPHEM_CREDS_ADDR'] if ephemeral else _os.environ['TBE_CREDS_ADDR']\n\u001b[1;32m    123\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    125\u001b[0m         'request_auth', request={'authType': 'dfs_ephemeral'}, timeout_sec=None)\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   request_id = send_request(\n\u001b[1;32m    170\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    100\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    101\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"]}]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":20125,"status":"ok","timestamp":1674191492420,"user":{"displayName":"Dev Kshatrainfotech","userId":"00316704694514822481"},"user_tz":-330},"id":"7Wj71eoo5xSh","colab":{"base_uri":"https://localhost:8080/"},"outputId":"16be5332-ad54-4f63-b9a0-23af4a17c055"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q rasterio\n","!pip install -q icecream\n","!pip install -q split-folders"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Khantil/Dump_Detection/dump_detection_shared/yolov7"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tiNCsRuejG90","executionInfo":{"status":"ok","timestamp":1674191511321,"user_tz":-330,"elapsed":1969,"user":{"displayName":"Dev Kshatrainfotech","userId":"00316704694514822481"}},"outputId":"03d71e4a-aa09-45fb-a825-771dfcfa707c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Khantil/Dump_Detection/dump_detection_shared/yolov7\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KgqMavc25yCp"},"outputs":[],"source":["import rasterio\n","import numpy as np\n","import rasterio\n","# from rasterio.plot import show\n","from rasterio.windows import Window\n","import cv2\n","import numpy as np\n","from matplotlib import pyplot as plt\n","import torch\n","import json\n","import glob\n","import os\n","from osgeo import gdal\n","from icecream import ic\n","from custom_scripts.data_augmentation import *\n","import splitfolders\n","from config import *"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MRaOwHQnRx9a"},"outputs":[],"source":["class yolo_train():\n","  def __init__(self):\n","    mode = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    if str(mode) == 'cuda':\n","      self.device = 0\n","      print(\"GPU Found..\")\n","    else:\n","      self.device = 'cpu'\n","      print(\"GPU not found, selected CPU.\")\n","    self.batch_size = batch_size_\n","    self.epoch = epoch_\n","    self.im_size = im_size_\n","    \n","    self.hyperparameter = hyperparameter_path\n","    self.configration = configration_path\n","    self.pretrained_weights = pretrained_weights_path\n","  def start_training(self, data_file_name, saving_name, model_saving_path):\n","    name_of_model = saving_name\n","    data_file = data_file_name\n","    custom_dir = model_saving_path\n","    !python3 train.py --device {self.device} --batch-size {self.batch_size} --epochs {self.epoch} --img {self.im_size} {self.im_size} --data {data_file} --hyp {self.hyperparameter} --cfg {self.configration} --weights {self.pretrained_weights} --name {name_of_model} --custom_dir {custom_dir}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KzG5hGGx54_S"},"outputs":[],"source":["class draw_bbox():\n","    def __init__(self):\n","        self.yolo = yolo_train()\n","\n","\n","    # This Function is For Display Image\n","    def show1(self, img):\n","        plt.figure(figsize=(9,13))\n","        plt.axis(\"off\")\n","        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n","        plt.show()\n","  \n","\n","\n","    # This Function Fetch Only Coordinate From Json File For Further Use\n","    # Input of this function is GeoJson file\n","\n","# --------------------------------------------------------------\n","    def read_coordinates(self, json_path):\n","        data = json.load(open(json_path))\n","        final_data = []\n","        for i in data[\"features\"]:\n","            tmp = {\"id\": \"\",\n","                  \"boxes\": \"\"}\n","            id_ = i[\"properties\"][\"id\"]\n","            box_coordinates = i[\"geometry\"][\"coordinates\"][0][:4]\n","            tmp[\"id\"], tmp[\"boxes\"] = id_, box_coordinates\n","            final_data.append(tmp)\n","        return final_data\n","\n","\n","\n","    def saving_coordinates(self, json_path):\n","        data = json.load(open(json_path))\n","        final_data = []\n","        for i in data[\"features\"]:\n","          tmp = {\"id\": \"\",\n","                  \"boxes\": \"\"}\n","          id_ = i[\"id\"]\n","          box_coordinates = i[\"coordinates\"][::-1]\n","          tmp[\"id\"], tmp[\"boxes\"] = id_, box_coordinates\n","          final_data.append(tmp)\n","        return final_data\n","\n","# ---------------------------------------------------------------\n","\n","    \n","    # This Function Return Matched Coordinate Between Start Point to End Point\n","    def get_all_boxes(self, final_list, start_x, end_x, start_y, end_y):\n","        \n","        fetched_bbox = []\n","        for i in final_list:\n","            tmp = {\"id\": \"\",\n","                \"boxes\": \"\"}\n","            if (i[\"boxes\"][0][0] >= start_x and i[\"boxes\"][2][0] <= end_x) and (i[\"boxes\"][0][1] >= end_y and i[\"boxes\"][2][1] <= start_y):\n","                tmp[\"id\"], tmp[\"boxes\"] = i[\"id\"], i[\"boxes\"]\n","                fetched_bbox.append(tmp)  \n","        return fetched_bbox\n","\n","\n","    \n","   \n","    # This Function Convert Real World Coordinate to image pixel\n","    # meta will used for transform coordinate to pixel\n","    def get_img_pixel_from_json(self, fetched_bbox_list, meta):\n","        converted_data = []\n","        for i in fetched_bbox_list:\n","            tmp = {\"id\": \"\",\n","                \"boxes\": \"\"}\n","            tmp[\"id\"] = i[\"id\"]\n","            dd = []\n","            for k in i[\"boxes\"]:\n","                x_coord = k[0]\n","                y_coord = k[1]\n","\n","                rowcol = rasterio.transform.rowcol(meta['transform'], xs=x_coord, ys=y_coord)\n","                y = rowcol[0]\n","                x = rowcol[1]\n","                dd.append([x,y])\n","            tmp[\"boxes\"] = dd\n","            converted_data.append(tmp)\n","        return converted_data\n","\n","\n","    # Convert bbox value to YOLO value\n","    # Accept Value in touple\n","    # size = (img_width, img_height)\n","    # box = (x,x+w, y, y+h)\n","    def bbox_to_yolo(self, size, box):\n","        dw = 1./size[0]\n","        dh = 1./size[1]\n","        x = (box[0] + box[1])/2.0\n","        y = (box[2] + box[3])/2.0\n","        w = box[1] - box[0]\n","        h = box[3] - box[2]\n","        x = x*dw\n","        w = w*dw\n","        y = y*dh\n","        h = h*dh\n","        return (x,y,w,h)\n","    \n","    \n","    # this function convert cropped image in proper image format and set channel for better visulization.\n","    # this function receive cropped image part and return image with drawed bounding box on image\n","    def convert_img_array(self, data, final_list, start_x, start_y, step_x, step_y, save_path, txt_number):\n","        save_path = save_path.replace(\"images\", \"\")\n","        save_path = os.path.join(save_path, \"labels\")\n","        if not os.path.exists(save_path):\n","          os.makedirs(save_path)\n","\n","        flag = False\n","        end_x = start_x + step_x\n","        end_y = start_y + step_y\n","        data = torch.from_numpy(data)\n","        data=data.permute(1, 2, 0)\n","        data = data.numpy()\n","        data = cv2.cvtColor(data, cv2.COLOR_BGR2RGB)\n","        height, width = data.shape[0], data.shape[1]\n","        for i in final_list:\n","            if (i[\"boxes\"][3][0] >= start_x) and (i[\"boxes\"][1][0] <= end_x) and (i[\"boxes\"][3][1] >= start_y) and (i[\"boxes\"][1][1] <= end_y):\n","                x0 = i[\"boxes\"][3][0] - start_x\n","                y0 = i[\"boxes\"][3][1] - start_y\n","                x2 = i[\"boxes\"][1][0] - start_x\n","                y2 = i[\"boxes\"][1][1] - start_y\n","                bbox_width = x2 - x0\n","                bbox_height = y2 - y0\n","                # cv2.rectangle(data, (x0,y0), (x0+bbox_width, y0+bbox_height), (255,0,0), 3)\n","\n","                x,y,w,h = self.bbox_to_yolo((width, height), (x0, x0+bbox_width, y0, y0+bbox_height))\n","\n","                file1 = open(f\"{save_path}/img_{txt_number}.txt\",\"a\")\n","                file1.write(f\"0 {x} {y} {w} {h} \\n\")\n","                file1.close()\n","                if flag == False:\n","                    flag = True\n","            else:\n","                pass\n","        return data, flag\n","\n","\n","    \n","    \n","    # this function cut images in sub parts and visulize box on images.\n","    def crop_and_draw_bbox(self, height, width, crop_size, converted_data, dataset, saving_path, show_box):\n","        cnt = 0\n","        step = crop_size\n","        width_cut = crop_size\n","        height_cut = crop_size\n","        saving_path = os.path.join(saving_path, \"images\")\n","        if not os.path.exists(saving_path):\n","            os.makedirs(saving_path)\n","\n","\n","        for col in range(0, height, step):\n","            if col + step >= height:\n","                height_cut = height - col\n","\n","            for row in range(0, width, step):\n","                if row+width_cut >= width:\n","                    final_cut = width - row\n","                    data = dataset.read([1,2,3], window=Window(row, col, final_cut, height_cut))\n","                    final_img, flag = self.convert_img_array(data, converted_data, row, col, final_cut, height_cut, saving_path, cnt)\n","                    if flag != False:\n","                        if show_box == True:\n","                            self.show1(final_img)\n","                            cv2.imwrite(f'{saving_path}/img_{cnt}.png', final_img)\n","                        else:\n","                            cv2.imwrite(f'{saving_path}/img_{cnt}.png', final_img)\n","                    cnt += 1\n","\n","                else:\n","                    data = dataset.read([1,2,3], window=Window(row, col, width_cut, height_cut))\n","                    final_img, flag = self.convert_img_array(data, converted_data, row, col, width_cut, height_cut, saving_path, cnt)\n","                    if flag != False:\n","                        if show_box == True:\n","                            self.show1(final_img)\n","                            cv2.imwrite(f'{saving_path}/img_{cnt}.png', final_img)\n","                        else:\n","                            cv2.imwrite(f'{saving_path}/img_{cnt}.png', final_img)\n","                    cnt += 1\n","\n","\n","\n","\n","    # by default show_bbox = False, it means function will save images to destination folder,\n","    # if you want to visulize image while running then set it to True. it will show images while running.\n","    def main(self, tiff_file_path, geojson_file_path, img_saving_path, model_dir, cropping_size = 2500, show_bbox = False, aug = False):\n","        if not os.path.exists(img_saving_path):\n","            os.makedirs(img_saving_path)\n","            print(\"Saving Directory Not Found.... \\n Created Saving Directory.....\")\n","        else:\n","            print(\"Saving Directory Found....\")\n","        cnt_ = 0\n","        tiff_file = tiff_file_path\n","        ds = gdal.Open(tiff_file)\n","        dataset = rasterio.open(tiff_file)\n","        meta_ = dataset.meta\n","\n","        width = ds.RasterXSize\n","        height = ds.RasterYSize\n","\n","        print(\"Height = \", height, \"Width =\", width)\n","        gt = ds.GetGeoTransform()\n","\n","        minx = gt[0]\n","        miny = gt[3]\n","        maxy = gt[3] + width*gt[4] + height*gt[5] \n","        maxx = gt[0] + width*gt[1] + height*gt[2]\n","\n","        \n","        try:\n","            final_list = self.read_coordinates(geojson_file_path)\n","            print(\"Approaching First Format\")  \n","        except:\n","            final_list = self.saving_coordinates(geojson_file_path)\n","            print(\"Approaching Second Format\")\n","        fetched_bbox = self.get_all_boxes(final_list, minx, maxx, miny, maxy)\n","        converted_data = self.get_img_pixel_from_json(fetched_bbox, meta_)\n","        self.crop_and_draw_bbox(height, width, cropping_size, converted_data, dataset, img_saving_path, show_bbox)\n","        if aug:\n","            aug = generateData() \n","\n","            data_path = os.path.join(img_saving_path, \"images\")\n","            image_save_path = os.path.join(img_saving_path, \"images/\")\n","            txt_save_path = os.path.join(img_saving_path, \"labels/\")\n","\n","            aug.augmentData(data_path, image_save_path, txt_save_path)\n","\n","        yolo_train_data_dir = \"yolo_training_data\"\n","        splitfolders.ratio(img_saving_path, output=f\"data/{yolo_train_data_dir}\", seed=1337, ratio=(.8, 0.2))\n","\n","        yaml_file_name = \"yolov7_custom.txt\"\n","        with open(f\"data/{yaml_file_name}\", \"w\") as file:\n","          file.write(f\"train: data/{yolo_train_data_dir}/train \\nval: data/{yolo_train_data_dir}/val \\nnc: 1 \\nnames: ['pile']\")\n","\n","\n","        rename_path = os.path.join(\"data\", yaml_file_name)\n","        new_name = rename_path.replace(\".txt\", \".yaml\")\n","        os.rename(rename_path, new_name)\n","\n","        model_saving_name = \"model1\"\n","        self.yolo.start_training(new_name, model_saving_name, model_dir)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YxOLFGof6Uqt"},"outputs":[],"source":["tiff_file1 = \"/content/drive/MyDrive/Khantil/Dump_Detection/dump_detection_shared/data/NamarDaharat2_Orthomosaic_export_TueAug23092831065915.tif\"\n","\n","\n","geo_data = \"/content/drive/MyDrive/Khantil/Dump_Detection/dump_detection_shared/geojson/NamarDaharat2_Orthomosaic_export_TueAug23092831065915.geojson\"\n","save_path = \"preprocessed_data\"\n","yolo_model_saving_path = \"yolo_sample_testing_modeln\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k00RYrq4733v","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1674192837191,"user_tz":-330,"elapsed":28357,"user":{"displayName":"Dev Kshatrainfotech","userId":"00316704694514822481"}},"outputId":"6bbbb8b4-6c30-42e0-a0e6-7e5479c05236"},"outputs":[{"output_type":"stream","name":"stdout","text":["GPU Found..\n","Saving Directory Not Found.... \n"," Created Saving Directory.....\n","Height =  34326 Width = 47417\n","Approaching Second Format\n","Max Number of Crops Reached...\n","Max Limit Reached...\n"]},{"output_type":"stream","name":"stderr","text":["Copying files: 10 files [00:00, 50.12 files/s]\n"]},{"output_type":"stream","name":"stdout","text":["YOLOR 🚀 2022-11-30 torch 1.13.1+cu116 CUDA:0 (A100-SXM4-40GB, 40536.1875MB)\n","\n","Namespace(adam=False, artifact_alias='latest', batch_size=16, bbox_interval=-1, bucket='', cache_images=False, cfg='cfg/training/yolov7_custom.yaml', custom_dir='yolo_sample_testing_modeln', data='data/yolov7_custom.yaml', device='0', entity=None, epochs=70, evolve=False, exist_ok=False, freeze=[0], global_rank=-1, hyp='data/hyp.scratch.custom.yaml', image_weights=False, img_size=[640, 640], label_smoothing=0.0, linear_lr=False, local_rank=-1, multi_scale=False, name='model1', noautoanchor=False, nosave=False, notest=False, project='runs/train', quad=False, rect=False, resume=False, save_dir='yolo_sample_testing_modeln', save_period=-1, single_cls=False, sync_bn=False, total_batch_size=16, upload_dataset=False, v5_metric=False, weights='yolov7.pt', workers=8, world_size=1)\n","\u001b[34m\u001b[1mtensorboard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n","2023-01-20 05:33:34.739128: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.3, cls_pw=1.0, obj=0.7, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.2, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, paste_in=0.0, loss_ota=1\n","\u001b[34m\u001b[1mwandb: \u001b[0mInstall Weights & Biases for YOLOR logging with 'pip install wandb' (recommended)\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1       928  models.common.Conv                      [3, 32, 3, 1]                 \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n","  5                -2  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n","  6                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n","  7                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n","  8                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n","  9                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n"," 10  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 11                -1  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n"," 12                -1  1         0  models.common.MP                        []                            \n"," 13                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 14                -3  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 16          [-1, -3]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 18                -2  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 19                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 20                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 21                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 22                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 23  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 24                -1  1    263168  models.common.Conv                      [512, 512, 1, 1]              \n"," 25                -1  1         0  models.common.MP                        []                            \n"," 26                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 27                -3  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 28                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 29          [-1, -3]  1         0  models.common.Concat                    [1]                           \n"," 30                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 31                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 32                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 33                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 34                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 35                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 36  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 37                -1  1   1050624  models.common.Conv                      [1024, 1024, 1, 1]            \n"," 38                -1  1         0  models.common.MP                        []                            \n"," 39                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n"," 40                -3  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n"," 41                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n"," 42          [-1, -3]  1         0  models.common.Concat                    [1]                           \n"," 43                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n"," 44                -2  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n"," 45                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 46                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 47                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 48                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 49  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 50                -1  1   1050624  models.common.Conv                      [1024, 1024, 1, 1]            \n"," 51                -1  1   7609344  models.common.SPPCSPC                   [1024, 512, 1]                \n"," 52                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 53                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 54                37  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n"," 55          [-1, -2]  1         0  models.common.Concat                    [1]                           \n"," 56                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 57                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 58                -1  1    295168  models.common.Conv                      [256, 128, 3, 1]              \n"," 59                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 60                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 61                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 62[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 63                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n"," 64                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 65                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 66                24  1     65792  models.common.Conv                      [512, 128, 1, 1]              \n"," 67          [-1, -2]  1         0  models.common.Concat                    [1]                           \n"," 68                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 69                -2  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 70                -1  1     73856  models.common.Conv                      [128, 64, 3, 1]               \n"," 71                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n"," 72                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n"," 73                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n"," 74[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 75                -1  1     65792  models.common.Conv                      [512, 128, 1, 1]              \n"," 76                -1  1         0  models.common.MP                        []                            \n"," 77                -1  1     16640  models.common.Conv                      [128, 128, 1, 1]              \n"," 78                -3  1     16640  models.common.Conv                      [128, 128, 1, 1]              \n"," 79                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 80      [-1, -3, 63]  1         0  models.common.Concat                    [1]                           \n"," 81                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 82                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 83                -1  1    295168  models.common.Conv                      [256, 128, 3, 1]              \n"," 84                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 85                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 86                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 87[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 88                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n"," 89                -1  1         0  models.common.MP                        []                            \n"," 90                -1  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n"," 91                -3  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n"," 92                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 93      [-1, -3, 51]  1         0  models.common.Concat                    [1]                           \n"," 94                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n"," 95                -2  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n"," 96                -1  1   1180160  models.common.Conv                      [512, 256, 3, 1]              \n"," 97                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 98                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 99                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n","100[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n","101                -1  1   1049600  models.common.Conv                      [2048, 512, 1, 1]             \n","102                75  1    328704  models.common.RepConv                   [128, 256, 3, 1]              \n","103                88  1   1312768  models.common.RepConv                   [256, 512, 3, 1]              \n","104               101  1   5246976  models.common.RepConv                   [512, 1024, 3, 1]             \n","105   [102, 103, 104]  1     34156  models.yolo.IDetect                     [1, [[12, 16, 19, 36, 40, 28], [36, 75, 76, 55, 72, 146], [142, 110, 192, 243, 459, 401]], [256, 512, 1024]]\n","Model Summary: 415 layers, 37196556 parameters, 37196556 gradients\n","\n","Transferred 552/566 items from yolov7.pt\n","Scaled weight_decay = 0.0005\n","Optimizer groups: 95 .bias, 95 conv.weight, 98 other\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'data/yolo_training_data/train/labels.cache' images and labels... 4 found, 0 missing, 0 empty, 0 corrupted: 100% 4/4 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning 'data/yolo_training_data/val/labels.cache' images and labels... 1 found, 0 missing, 0 empty, 0 corrupted: 100% 1/1 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 4.80, Best Possible Recall (BPR) = 1.0000\n","Image sizes 640 train, 640 test\n","Using 4 dataloader workers\n","Logging results to yolo_sample_testing_modeln\n","Starting training for 70 epochs...\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","      0/69     38.1G   0.09237   0.03944         0    0.1318       102       640: 100% 1/1 [00:09<00:00,  9.60s/it]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95:   0% 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 1/1 [00:00<00:00,  1.05it/s]\n","                 all           1           0           0           0           0           0\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","      1/69     38.1G   0.05668   0.04778         0    0.1045       110       640: 100% 1/1 [00:00<00:00,  5.83it/s]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 1/1 [00:00<00:00,  4.81it/s]\n","                 all           1           0           0           0           0           0\n","\n","^C\n"]}],"source":["obj = draw_bbox() \n","\n","# default cropping_size = 2500, show_bbox = False, aug(augmentation) = False\n","obj.main(tiff_file1, geo_data, save_path, yolo_model_saving_path, cropping_size = 2500, show_bbox = False, aug = False) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gYOiIpvWiG61"},"outputs":[],"source":[]},{"cell_type":"code","source":[],"metadata":{"id":"QFDiEcVxlqE_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"EVTh75Avlp91"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"OCmCfdbnlp6t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"WzHMixM0lpyt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-57CxZIBlpwI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"kHjYq7VAptJx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"wYR5CORGptG4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oHZ2iCCvptB7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"DxjEu7Xqps_b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"aUeJ9faOps9F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# sample testing function\n","def crop_and_draw_bbox(self, height, width, crop_size, converted_data, dataset, saving_path, show_box):\n","        cnt = 0\n","        step = crop_size\n","        width_cut = crop_size\n","        height_cut = crop_size\n","\n","        crop_number = 5\n","        max_count_flag = False\n","\n","        saving_path = os.path.join(saving_path, \"images\")\n","        if not os.path.exists(saving_path):\n","            os.makedirs(saving_path)\n","\n","\n","        for col in range(0, height, step):\n","            if max_count_flag == True:\n","              print(\"Max Limit Reached...\")\n","              break\n","            else:\n","              \n","              if col + step >= height:\n","                  height_cut = height - col\n","\n","              for row in range(0, width, step):\n","                  if row+width_cut >= width:\n","                      final_cut = width - row\n","                      data = dataset.read([1,2,3], window=Window(row, col, final_cut, height_cut))\n","                      final_img, flag = self.convert_img_array(data, converted_data, row, col, final_cut, height_cut, saving_path, cnt)\n","                      if flag != False:\n","                          if show_box == True:\n","                              self.show1(final_img)\n","                              cv2.imwrite(f'{saving_path}/img_{cnt}.png', final_img)\n","                          else:\n","                              cv2.imwrite(f'{saving_path}/img_{cnt}.png', final_img)\n","                      if cnt == crop_number:\n","                          print(\"Max Number of Crops Reached...\")\n","                          max_count_flag = True\n","                          break\n","                      cnt += 1\n","\n","                  else:\n","                      data = dataset.read([1,2,3], window=Window(row, col, width_cut, height_cut))\n","                      final_img, flag = self.convert_img_array(data, converted_data, row, col, width_cut, height_cut, saving_path, cnt)\n","                      if flag != False:\n","                          if show_box == True:\n","                              self.show1(final_img)\n","                              cv2.imwrite(f'{saving_path}/img_{cnt}.png', final_img)\n","                          else:\n","                              cv2.imwrite(f'{saving_path}/img_{cnt}.png', final_img)\n","                      if cnt == crop_number:\n","                          print(\"Max Number of Crops Reached...\")\n","                          max_count_flag = True\n","                          break\n","                      cnt += 1"],"metadata":{"id":"X6ULdcuYlpsr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# original function \n","def crop_and_draw_bbox(self, height, width, crop_size, converted_data, dataset, saving_path, show_box):\n","        cnt = 0\n","        step = crop_size\n","        width_cut = crop_size\n","        height_cut = crop_size\n","        saving_path = os.path.join(saving_path, \"images\")\n","        if not os.path.exists(saving_path):\n","            os.makedirs(saving_path)\n","\n","\n","        for col in range(0, height, step):\n","            if col + step >= height:\n","                height_cut = height - col\n","\n","            for row in range(0, width, step):\n","                if row+width_cut >= width:\n","                    final_cut = width - row\n","                    data = dataset.read([1,2,3], window=Window(row, col, final_cut, height_cut))\n","                    final_img, flag = self.convert_img_array(data, converted_data, row, col, final_cut, height_cut, saving_path, cnt)\n","                    if flag != False:\n","                        if show_box == True:\n","                            self.show1(final_img)\n","                            cv2.imwrite(f'{saving_path}/img_{cnt}.png', final_img)\n","                        else:\n","                            cv2.imwrite(f'{saving_path}/img_{cnt}.png', final_img)\n","                    cnt += 1\n","\n","                else:\n","                    data = dataset.read([1,2,3], window=Window(row, col, width_cut, height_cut))\n","                    final_img, flag = self.convert_img_array(data, converted_data, row, col, width_cut, height_cut, saving_path, cnt)\n","                    if flag != False:\n","                        if show_box == True:\n","                            self.show1(final_img)\n","                            cv2.imwrite(f'{saving_path}/img_{cnt}.png', final_img)\n","                        else:\n","                            cv2.imwrite(f'{saving_path}/img_{cnt}.png', final_img)\n","                    cnt += 1"],"metadata":{"id":"_3Vgs2cmlppm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# stack link\n","# https://stackoverflow.com/questions/74372636/indices-should-be-either-on-cpu-or-on-the-same-device-as-the-indexed-tensor"],"metadata":{"id":"edNEYWy0lpnS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"wKrCF2rQshQB"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":0}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17530,
     "status": "ok",
     "timestamp": 1670244718898,
     "user": {
      "displayName": "Dev Kshatrainfotech",
      "userId": "00316704694514822481"
     },
     "user_tz": -330
    },
    "id": "EnLIRi7hw66c",
    "outputId": "fccda49d-30ff-47ca-c3d1-93ee7ee7ec38"
   },
   "outputs": [],
   "source": [
    "# !pip install -q rasterio\n",
    "# !pip install -q icecream\n",
    "# !pip install -q gdal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 1071,
     "status": "ok",
     "timestamp": 1670245632411,
     "user": {
      "displayName": "Dev Kshatrainfotech",
      "userId": "00316704694514822481"
     },
     "user_tz": -330
    },
    "id": "oJInvKHIwIfE"
   },
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import os\n",
    "# from rasterio.plot import show\n",
    "from rasterio.windows import Window\n",
    "import cv2\n",
    "from osgeo import gdal\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import argparse\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "import torch.backends.cudnn as cudnn\n",
    "from numpy import random\n",
    "from icecream import ic\n",
    "\n",
    "# import sys\n",
    "# sys.path.append(\"./yolov7\")\n",
    "from models.experimental import attempt_load\n",
    "from utils.datasets import LoadStreams, LoadImages\n",
    "from utils.general import check_img_size, check_requirements, check_imshow, non_max_suppression, apply_classifier, \\\n",
    "    scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path\n",
    "from utils.plots import plot_one_box\n",
    "from utils.torch_utils import select_device, load_classifier, time_synchronized, TracedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1670245632412,
     "user": {
      "displayName": "Dev Kshatrainfotech",
      "userId": "00316704694514822481"
     },
     "user_tz": -330
    },
    "id": "4PMR0wN9DfSX"
   },
   "outputs": [],
   "source": [
    "class detect_piles():\n",
    "  def __init__(self, weights):\n",
    "    self.device = select_device('')\n",
    "\n",
    "    # load FP32 model\n",
    "    self.model = attempt_load(weights, map_location=self.device)\n",
    "    print(\"Model Loaded\")\n",
    "\n",
    "\n",
    "  def detect(self, source_path, pred_saving_path, imgsize = 640, view_img_flag = False, save_img=False, trace_flag = False):\n",
    "\n",
    "    all_boxes = []\n",
    "\n",
    "    conf_thres = 0.5\n",
    "    iou_thres = 0.45\n",
    "    classes = None\n",
    "    agnostic_nms = False\n",
    "    save_conf = False\n",
    "    save_txt = True\n",
    "    nosave = False\n",
    "    save_dir = pred_saving_path\n",
    "\n",
    "    source, view_img, imgsz, trace = source_path, view_img_flag, imgsize, trace_flag\n",
    "    \n",
    "    save_img = not nosave and not source.endswith('.txt')\n",
    "    # webcam = source.isnumeric() or source.endswith('.txt') or source.lower().startswith(('rtsp://', 'rtmp://', 'http://', 'https://'))\n",
    "    webcam = False\n",
    "    set_logging()\n",
    "    half = self.device.type != 'cpu'\n",
    "\n",
    "      \n",
    "    stride = int(self.model.stride.max())  # model stride\n",
    "    imgsz = check_img_size(imgsz, s=stride) \n",
    "    img_size = 640\n",
    "    \n",
    "    if trace:\n",
    "      self.model = TracedModel(self.model, self.device, img_size)\n",
    "    if half:\n",
    "      self.model.half()\n",
    "\n",
    "    classify = False\n",
    "\n",
    "    if classify:\n",
    "      modelc = load_classifier(name='resnet101', n=2)  # initialize\n",
    "      modelc.load_state_dict(torch.load('weights/resnet101.pt', map_location=self.device)['model']).to(self.device).eval()\n",
    "\n",
    "    vid_path, vid_writer = None, None\n",
    "\n",
    "    if webcam:\n",
    "      view_img = check_imshow()\n",
    "      cudnn.benchmark = True  # set True to speed up constant image size inference\n",
    "      dataset = LoadStreams(source, img_size=imgsz, stride=stride)\n",
    "\n",
    "    else:\n",
    "      dataset = LoadImages(source, img_size=imgsz, stride=stride)\n",
    "    \n",
    "    names = self.model.module.names if hasattr(self.model, 'module') else self.model.names\n",
    "    # colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n",
    "    colors = [[0,0,255]]\n",
    "\n",
    "    if self.device.type != 'cpu':\n",
    "      self.model(torch.zeros(1, 3, imgsz, imgsz).to(self.device).type_as(next(self.model.parameters())))  # run once\n",
    "    old_img_w = old_img_h = imgsz\n",
    "    old_img_b = 1\n",
    "    augment = False\n",
    "    t0 = time.time()\n",
    "\n",
    "    for path, img, im0s, vid_cap in dataset:\n",
    "      img = torch.from_numpy(img).to(self.device)\n",
    "      img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "      img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "\n",
    "      if img.ndimension() == 3:\n",
    "        img = img.unsqueeze(0)\n",
    "\n",
    "      if self.device.type != 'cpu' and (old_img_b != img.shape[0] or old_img_h != img.shape[2] or old_img_w != img.shape[3]):\n",
    "        old_img_b = img.shape[0]\n",
    "        old_img_h = img.shape[2]\n",
    "        old_img_w = img.shape[3]\n",
    "\n",
    "        for i in range(3):\n",
    "          self.model(img, augment=augment)[0]\n",
    "\n",
    "      t1 = time_synchronized()\n",
    "\n",
    "      with torch.no_grad():     # Calculating gradients would cause a GPU memory leak\n",
    "        pred = self.model(img, augment=augment)[0]\n",
    "      t2 = time_synchronized()\n",
    "      pred = non_max_suppression(pred, conf_thres, iou_thres, classes=classes, agnostic=agnostic_nms)\n",
    "      t3 = time_synchronized()\n",
    "\n",
    "      if classify:\n",
    "        pred = apply_classifier(pred, modelc, img, im0s)\n",
    "\n",
    "      for i, det in enumerate(pred):\n",
    "        if webcam:\n",
    "          p, s, im0, frame = path[i], '%g: ' % i, im0s[i].copy(), dataset.count\n",
    "        else:\n",
    "          p, s, im0, frame = path, '', im0s, getattr(dataset, 'frame', 0)\n",
    "        p = Path(p)\n",
    "        save_path = save_dir + \"/\" + p.name\n",
    "        # txt_path = str(save_dir + \"/\" + p.stem) + ('' if dataset.mode == 'image' else f'_{frame}')\n",
    "        gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "        if len(det):\n",
    "          det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
    "          for c in det[:, -1].unique():\n",
    "            n = (det[:, -1] == c).sum()\n",
    "            s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"\n",
    "          for *xyxy, conf, cls in reversed(det):\n",
    "            if save_txt:\n",
    "              xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()\n",
    "              all_boxes.append([xywh[0], xywh[1], xywh[2], xywh[3]])\n",
    "              line = (cls, *xywh, conf) if save_conf else (cls, *xywh)\n",
    "              # with open(txt_path + '.txt', 'a') as f:\n",
    "              #   f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n",
    "\n",
    "            if save_img or view_img:\n",
    "              label = f'{names[int(cls)]} {conf:.2f}'\n",
    "              plot_one_box(xyxy, im0, label=label, color=colors[int(cls)], line_thickness=1)\n",
    "        # print(f'{s}Done. ({(1E3 * (t2 - t1)):.1f}ms) Inference, ({(1E3 * (t3 - t2)):.1f}ms) NMS')\n",
    "        if view_img:\n",
    "          cv2.imshow(str(p), im0)\n",
    "          cv2.waitKey(1)\n",
    "        ww, hh = im0.shape[1], im0.shape[0]  \n",
    "        if save_img:\n",
    "          if dataset.mode == 'image':\n",
    "            cv2.imwrite(save_path, im0)\n",
    "            print(f\" The image with the result is saved in: {save_path}\")\n",
    "          else: \n",
    "            if vid_path != save_path:\n",
    "              vid_path = save_path\n",
    "              if isinstance(vid_writer, cv2.VideoWriter):\n",
    "                vid_writer.release()\n",
    "              if vid_cap:\n",
    "                fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
    "                w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "                h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "              else: \n",
    "                fps, w, h = 30, im0.shape[1], im0.shape[0]\n",
    "                save_path += '.mp4'\n",
    "              vid_writer = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
    "            vid_writer.write(im0)  \n",
    "    return all_boxes, ww, hh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1670245632414,
     "user": {
      "displayName": "Dev Kshatrainfotech",
      "userId": "00316704694514822481"
     },
     "user_tz": -330
    },
    "id": "1wML1evuA6wa"
   },
   "outputs": [],
   "source": [
    "class get_predictions():\n",
    "  def __init__(self, w_path):\n",
    "    self.pile_class = detect_piles(w_path)\n",
    "\n",
    "  def convert_img_array(self, data):\n",
    "    data = torch.from_numpy(data)\n",
    "    data=data.permute(1, 2, 0)\n",
    "    data = data.numpy()\n",
    "    data = cv2.cvtColor(data, cv2.COLOR_BGR2RGB)\n",
    "    return data\n",
    "\n",
    "  def yolo2bbox(self, x, y, w, h, img_height, img_width):\n",
    "    x = float(x)\n",
    "    y = float(y)\n",
    "    w = float(w)\n",
    "    h = float(h)\n",
    "    img_height = float(img_height)\n",
    "    img_width = float(img_width)\n",
    "    height = h * img_height\n",
    "    width = w * img_width\n",
    "    left = (x * img_width) - (width // 2)\n",
    "    top = (y * img_height) - (height // 2)\n",
    "    return round(top), round(left), round(height), round(width)\n",
    "\n",
    "\n",
    "\n",
    "  def generate_crops(self, file_path, pred_path, tmp_folder, crop_size=2000):\n",
    "\n",
    "    if not os.path.exists(tmp_folder):\n",
    "      print(\"not found tmp folder.. \\n creating tmp folder...\")\n",
    "      os.makedirs(tmp_folder)\n",
    "    if not os.path.exists(pred_path):\n",
    "      print(f\"not found Prediction Saving Path.. \\n creating pred_path folder...\")\n",
    "      os.makedirs(pred_path)\n",
    "\n",
    "    file_name = file_path.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    img = rasterio.open(file_path)\n",
    "    profile = img.profile\n",
    "\n",
    "    width = profile[\"width\"]\n",
    "    height = profile[\"height\"]\n",
    "\n",
    "    print(\"Total Width = \", width, \"Total Height = \", height)\n",
    "\n",
    "    width_cut = crop_size\n",
    "    height_cut = crop_size\n",
    "    step = crop_size\n",
    "    cnt_ = 0\n",
    "    \n",
    "\n",
    "    for col in range(0, height, step):\n",
    "      if col + step >= height:\n",
    "        height_cut = height - col\n",
    "\n",
    "      for row in range(0, width, step):\n",
    "\n",
    "        if row+width_cut >= width:\n",
    "          final_cut = width - row\n",
    "          data = img.read([1,2,3], window=Window(row,col, final_cut, height_cut))\n",
    "          final_img = self.convert_img_array(data)\n",
    "          cv2.imwrite(f'{tmp_folder}/img_{cnt_}.png', final_img)\n",
    "          sample = f'{tmp_folder}/img_{cnt_}.png'\n",
    "          pred_boxes, img_w, img_h = self.pile_class.detect(sample, pred_path)\n",
    "          os.remove(sample)\n",
    "          cnt_ += 1\n",
    "\n",
    "        else:\n",
    "          data = img.read([1,2,3], window=Window(row,col, width_cut, height_cut))\n",
    "          final_img = self.convert_img_array(data)\n",
    "          cv2.imwrite(f'{tmp_folder}/img_{cnt_}.png', final_img)\n",
    "          sample = f'{tmp_folder}/img_{cnt_}.png'\n",
    "          pred_boxes, img_w, img_h = self.pile_class.detect(sample, pred_path)\n",
    "          os.remove(sample)\n",
    "          cnt_ += 1\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff_file1 = r\"D:\\Dropbox\\P.Robeck\\BPLA Dropbox\\03 Planning\\DN-Dumping Detection-1087\\03_Data\\Dronedeploy-exports\\aerial\\NamarDaharat1_Orthomosaic_TueAug23092916168449\\NamarDaharat1_Orthomosaic_export_TueAug23092916168449.tif\"\n",
    "\n",
    "save_pred_path = \"free_test/yolo_detected\"\n",
    "json_saving_path  = \"free_test/generated_geo_files\"\n",
    "tmp_file_path = \"free_test/tmp\" # use for Store images which will go to yolo model for prediction and then remove it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4324,
     "status": "ok",
     "timestamp": 1670245639494,
     "user": {
      "displayName": "Dev Kshatrainfotech",
      "userId": "00316704694514822481"
     },
     "user_tz": -330
    },
    "id": "mu9OZlN3HB9G",
    "outputId": "746706c6-b6e3-4690-83e1-50138c80f90e"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'weights_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m obj \u001b[39m=\u001b[39m get_predictions(weights_path)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'weights_path' is not defined"
     ]
    }
   ],
   "source": [
    "obj = get_predictions(weights_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 37148,
     "status": "error",
     "timestamp": 1670245725169,
     "user": {
      "displayName": "Dev Kshatrainfotech",
      "userId": "00316704694514822481"
     },
     "user_tz": -330
    },
    "id": "EQqyc_ghK-vF",
    "outputId": "5f3db094-8476-4a58-9854-92a06ea56d67"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'obj' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m obj\u001b[39m.\u001b[39mgenerate_crops(tiff_file1, save_pred_path, tmp_file_path)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'obj' is not defined"
     ]
    }
   ],
   "source": [
    "obj.generate_crops(tiff_file1, save_pred_path, tmp_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LETuw_tQTnmI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MGagCzef7ixA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOXQK7H45pImgWU47GwLyHF",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "f835f74e9034139cf60e8cf4ecdc7ebc2f1c85b2bf69cce491652cbaf7c45574"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
